## [An In-depth Performance Characterization of CPU- and GPU-based DNN Training on Modern Architectures](http://mvapich.cse.ohio-state.edu/static/media/talks/slide/awan-mlhpc17.pdf)

#### The authors propose a few questions about multi-node distributed training for deep learning models.

- What are the computation and communication characteristics of popular DL workloads?
- How various datasets and networks are handled differently in DL frameworks that execute on CPUs and/or GPUs?
- Can we devise any possible strategies to evaluate the performance of DL frameworks on different compute architectures in a standard manner?
- What are the performance trends that can be observed when only a single GPU or a single CPU/processor is used?
- How does the performance behavior change if certain hardware features like MCDRAM [7] are exploited?
- To what degree can scale-out ofDNNtraining help if multiple nodes for both CPU-based and GPU-based DNN training are utilized?
